{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant python packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import matplotlib.path as mpath\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import csv\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from scipy.signal import windows, convolve\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa79120",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a11700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# great-circle central angle (haversine); result in degrees\n",
    "def angular_distance_deg(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arcsin(np.minimum(1.0, np.sqrt(a)))\n",
    "    return np.degrees(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_csv_with_metadata(path):\n",
    "    \"\"\"\n",
    "    Reads a CSV whose first lines are metadata starting with '#',\n",
    "    followed by a header row and data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        The data table.\n",
    "    meta : dict\n",
    "        Parsed metadata (e.g., {'Lat': -34.666, 'Cruise': 'ABC123'}).\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "\n",
    "    # --- First pass: parse the metadata lines (those starting with '#') ---\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.lstrip()\n",
    "            if not s.startswith(\"#\"):\n",
    "                # first non-comment line should be the header; stop scanning\n",
    "                break\n",
    "\n",
    "            # Parse lines like \"# Lat = -34.666\" or \"#Lat: -34.666\"\n",
    "            m = re.match(r\"^\\s*#\\s*([^=:]+)\\s*[:=]\\s*(.*)\\s*$\", s)\n",
    "            if not m:\n",
    "                continue\n",
    "            key = m.group(1).strip()\n",
    "            val = m.group(2).strip().strip(\"'\\\"\")\n",
    "\n",
    "            # Try to coerce to number/bool if possible\n",
    "            num = pd.to_numeric(val, errors=\"coerce\")\n",
    "            if pd.notna(num):\n",
    "                # If it parses as a number, store as float (or int if exact)\n",
    "                meta[key] = int(num) if float(num).is_integer() else float(num)\n",
    "            else:\n",
    "                low = val.lower()\n",
    "                if low in (\"true\", \"false\"):\n",
    "                    meta[key] = (low == \"true\")\n",
    "                else:\n",
    "                    meta[key] = val\n",
    "\n",
    "    # --- Second pass: load the table (pandas ignores '#' lines via comment) ---\n",
    "    df = pd.read_csv(path, comment=\"#\")\n",
    "\n",
    "    # Optional: carry metadata along with the DataFrame\n",
    "    df.attrs.update(meta)\n",
    "\n",
    "    return df, meta\n",
    "\n",
    "# -----------------------\n",
    "# Example usage\n",
    "# -----------------------\n",
    "# path = \"your_file.csv\"\n",
    "# df, meta = read_csv_with_metadata(path)\n",
    "# print(df.head())\n",
    "# print(meta)\n",
    "\n",
    "# Access numeric metadata for calculations:\n",
    "# lat = float(meta.get(\"Lat\"))           # e.g., -34.666\n",
    "# lon = float(meta.get(\"Lon\", meta.get(\"Longitude\", float(\"nan\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the predict_SP function\n",
    "def predict_SP(t, p, gmm, train_data, samples=3000):\n",
    "    if isinstance(train_data, pd.DataFrame):\n",
    "        potential_SP = np.linspace(train_data['salinity'].min(), train_data['salinity'].max(), samples)\n",
    "    else:\n",
    "        raise ValueError(\"train_data should be a pandas DataFrame\")\n",
    "    \n",
    "    tp_sp_combinations = pd.DataFrame({\n",
    "        'temp': [t] * samples,\n",
    "        'pressure': [p] * samples,\n",
    "        'salinity': potential_SP\n",
    "    })\n",
    "        \n",
    "    #tp_sp_combinations = np.array([[t, p, sp] for sp in potential_SP])\n",
    "    log_likelihoods = gmm.score_samples(tp_sp_combinations)\n",
    "    best_sp = potential_SP[np.argmax(log_likelihoods)]\n",
    "    \n",
    "    return best_sp\n",
    "\n",
    "def predict_SP_proba(t, p, gmm, train_data, samples=3000):\n",
    "    if isinstance(train_data, pd.DataFrame):\n",
    "        potential_SP = np.linspace(train_data['salinity'].min(), train_data['salinity'].max(), samples)\n",
    "    else:\n",
    "        raise ValueError(\"train_data should be a pandas DataFrame\")\n",
    "    \n",
    "    tp_sp_combinations = pd.DataFrame({\n",
    "        'temp': [t] * samples,\n",
    "        'pressure': [p] * samples,\n",
    "        'salinity': potential_SP\n",
    "    })\n",
    "        \n",
    "    proba = gmm.predict_proba(tp_sp_combinations)\n",
    "    \n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07799989",
   "metadata": {},
   "source": [
    "# set details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd990c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Leg  = 'DG2401_L4_NCT'\n",
    "Station = 'NC4_OM11_4500'\n",
    "# Latitude and longitude tolerance\n",
    "deg_tolerance = 5\n",
    "year_tolerance = 10\n",
    "p_min = 1500\n",
    "##### these paths will need to be changed to run on another computer ####\n",
    "base_path_raw = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/EXT-MUDSRC - Documents/Cruises/2024_02_Nova Canton/Data/CTD/NC4/Landers/Processed/2_dbar_avg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7334adb",
   "metadata": {},
   "source": [
    "## file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define file paths and variables\n",
    "##### these paths will need to be changed to run on another computer ####\n",
    "base_path_argo = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/Working Nate/Summer Project/Data/Publically_available_data/Argo'\n",
    "base_path_cchdo = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/Working Nate/Summer Project/Data/Publically_available_data/CCHDO'\n",
    "base_path_good_dat = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/Working Nate/Summer Project/Data/Collected_data/Pressure_temp_salinity'\n",
    "\n",
    "figure_file = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/gmm_figs'\n",
    "folder_path_out = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/gmm_out'\n",
    "folder_matched_files_out = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/gmm_out/matched_profiles_details/'\n",
    "#filepath_out_btm = '/Users/jesskolbusz/Library/CloudStorage/OneDrive-SharedLibraries-TheUniversityofWesternAustralia/EXT-MUDSRC - Documents/PROJECTS/2024_JapanTrench/Data/Env/CTD/Processed/GMM/Output/Bottom'\n",
    "results_df = pd.DataFrame(columns=['Station', 'Number of Clusters', 'Type of Covariance', 'RMSE', 'Means', 'Covariances', 'Weights'])\n",
    "\n",
    "summary_argo_csv = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/argo/NOV_profiles_summary_over2500.csv'\n",
    "summary_cchdo_csv = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/NOV_profiles_cchdo_summary_over2500.csv'\n",
    "\n",
    "df_argo = pd.read_csv(summary_argo_csv)\n",
    "df_argo[\"time\"] = pd.to_datetime(df_argo[\"time\"], errors=\"coerce\")\n",
    "df_argo[\"year\"] = df_argo[\"time\"].dt.year\n",
    "\n",
    "df_cchdo = pd.read_csv(summary_cchdo_csv)\n",
    "df_cchdo[\"time\"] = pd.to_datetime(df_cchdo[\"time\"], errors=\"coerce\")\n",
    "df_cchdo[\"year\"] = df_cchdo[\"time\"].dt.year\n",
    "\n",
    "dep_log_file = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/2024_02_Nova_Canton_Oceanographic_Data.xlsx'\n",
    "dep_log = pd.read_excel(dep_log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49320795",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dep_log = dep_log[dep_log['Station'] == Station]\n",
    "\n",
    "target_lat = filtered_dep_log['Lat'].iloc[0]\n",
    "target_lon = filtered_dep_log['Lon'].iloc[0]\n",
    "target_year = filtered_dep_log['Date'].iloc[0].year  # <- set this to your reference date\n",
    "\n",
    "file_pred = os.path.join(base_path_raw, Station + '_processed_pub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get files within bounds for argo\n",
    "ang = angular_distance_deg(df_argo[\"lat\"].to_numpy(), df_argo[\"lon\"].to_numpy(), target_lat, target_lon)\n",
    "mask = ang <= deg_tolerance\n",
    "# years difference (approx using 365.25)\n",
    "year_diff = (df_argo[\"year\"] - target_year).abs()\n",
    "mask_time = year_diff <= year_tolerance\n",
    "\n",
    "### get files within bounds for cchdo\n",
    "mask_cchdo = mask & mask_time\n",
    "matched_files1 = df_argo.loc[mask_cchdo, \"profile_csv\"].dropna().unique()\n",
    "ang_cchdo = angular_distance_deg(df_cchdo[\"lat\"].to_numpy(), df_cchdo[\"lon\"].to_numpy(), target_lat, target_lon)\n",
    "mask_cchdo = ang_cchdo <= deg_tolerance\n",
    "# years difference (approx using 365.25)\n",
    "year_diff = (df_cchdo[\"year\"] - target_year).abs()\n",
    "mask_time_cchdo = year_diff <= year_tolerance\n",
    "mask_cchdo = mask_cchdo & mask_time_cchdo\n",
    "matched_files2 = df_cchdo.loc[mask_cchdo, \"profile_csv\"].dropna().unique()\n",
    "\n",
    "## all files together\n",
    "all_files_matched = np.concatenate([matched_files1, matched_files2])\n",
    "\n",
    "### Save matched filenames to CSV \n",
    "pd.DataFrame({\"file\": all_files_matched}).to_csv(os.path.join(folder_matched_files_out, \"matched_filenames_\" + Station + \".csv\"), index=False)\n",
    "print(f\"{len(all_files_matched)} files saved to matched_filenames_{Station}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc82986",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all the matched files \n",
    "dfs = (pd.read_csv(f) for f in all_files_matched)\n",
    "\n",
    "filtered_dfs = (\n",
    "    df[(df['pressure'] >= p_min) &(df['pressure_qc'] <= 2) & (df['temp_qc'] <= 2) & (df['salinity_qc'] <= 2)]\n",
    "    for df in dfs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5200b",
   "metadata": {},
   "source": [
    "# GMM run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GMM settings\n",
    "randstat = 42\n",
    "cov_type= 'full' #tied, diag, spherical \n",
    "tsize=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat(filtered_dfs, ignore_index=True)\n",
    "training_data.dropna(subset=['temp', 'pressure', 'salinity'], inplace=True)\n",
    "\n",
    "X_train = training_data[['temp', 'pressure', 'salinity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Initialise empty lists for varying numbers of clusters and \n",
    "## labels, AIC/BIc, etc. \n",
    "\n",
    "n_clusters = []\n",
    "labels_gmm = []\n",
    "centers_gmm = []\n",
    "aic_gmm = []\n",
    "bic_gmm = []\n",
    "\n",
    "k_clusters = range(1,15)\n",
    "\n",
    "for k in tqdm(k_clusters):\n",
    "    n_clusters.append(k)\n",
    "    ## Define the GMM model\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type= cov_type, random_state = randstat)\n",
    "    # Fit and predict the GMM to the control data\n",
    "    labels_gmm.append(gmm.fit_predict(np.array(X_train)))\n",
    "    centers_gmm.append(gmm.means_)\n",
    "    aic_gmm.append(gmm.aic(X_train))\n",
    "    bic_gmm.append(gmm.bic(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35aff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### elbow method input where changes #####\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 5))\n",
    "plt.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "\n",
    "axs.plot(n_clusters, np.array(bic_gmm)/10**6, 'r-o', label='BIC')\n",
    "axs.plot(n_clusters, np.array(aic_gmm)/10**6, 'b-o', label='AIC')\n",
    "axs.scatter(n_clusters[np.argmin(aic_gmm)], aic_gmm[np.argmin(aic_gmm)]/10**6, 50, color='blue', ec='k', zorder=10000)\n",
    "axs.scatter(n_clusters[np.argmin(bic_gmm)], bic_gmm[np.argmin(bic_gmm)]/10**6, 50, color='red', ec='k', zorder=10000)\n",
    "\n",
    "axs.set_xticks(n_clusters)\n",
    "axs.set_xlabel('Number of clusters')\n",
    "axs.set_xlim(1,14)\n",
    "#axs.set_ylim(-5.5,-3)\n",
    "\n",
    "axs.grid()\n",
    "axs.set_ylabel(r'AIC/BIC [$\\times10^6$]')\n",
    "axs.legend()\n",
    "\n",
    "plt.suptitle('Determination of optimal number of clusters', fontsize=12)\n",
    "plt.savefig(os.path.join(figure_file, Station + '_Scores_clusters_' + cov_type + '.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeeb70b",
   "metadata": {},
   "source": [
    "## input n clusters based on elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT VALUES ####\n",
    "n_components = 8# Example number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and evaluation sets - use 20% for evaluation\n",
    "train_data, eval_data = train_test_split(X_train, test_size=tsize, random_state=randstat)\n",
    "\n",
    "# Ensure they are DataFrames\n",
    "train_data = pd.DataFrame(train_data)\n",
    "train_data.columns = ['temp','pressure','salinity']\n",
    "\n",
    "eval_data = pd.DataFrame(eval_data)\n",
    "eval_data.columns = ['temp','pressure','salinity']\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(n_components=n_components, covariance_type= cov_type, random_state = randstat)\n",
    "gmm.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed8f0e",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc23284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming profile_data is a pandas DataFrame with columns 'Temp' and 'Pressure'\n",
    "## predict and get \n",
    "df, meta = read_csv_with_metadata(file_pred)\n",
    "\n",
    "df_new = df[['p','tnc90C']] \n",
    "df_new.columns = ['pressure','temp']\n",
    "df_new = df_new[(df_new['pressure'] >= 1500)]\n",
    "\n",
    "profile_data = df_new[['pressure','temp']]\n",
    "profile_data = profile_data.reset_index(drop=True)\n",
    "\n",
    "# Sample temperature and pressure data\n",
    "temperature = np.array(df_new['temp'])\n",
    "pressure = np.array(df_new['pressure'])\n",
    "\n",
    "# Stack temperature and pressure arrays column-wise\n",
    "# Initialize list to store predicted salinity values\n",
    "predicted_sps_profile = []\n",
    "predicted_sp_proba_prof = []\n",
    "\n",
    "# Iterate over the profile data to predict salinity\n",
    "for index, row in profile_data.iterrows():\n",
    "    t_real = row['temp']\n",
    "    p_real = row['pressure']\n",
    "    \n",
    "    # Predict salinity using the predict_SP function\n",
    "    predicted_sp = predict_SP(t_real, p_real, gmm, train_data)\n",
    "    predicted_sp_proba = predict_SP_proba(t_real, p_real, gmm, train_data)\n",
    "\n",
    "    # Append predicted salinity value to the list\n",
    "    predicted_sps_profile.append(predicted_sp)\n",
    "    predicted_sp_proba_prof.append(predicted_sp_proba)\n",
    "\n",
    "\n",
    "# If you have actual salinity values for profile_data, you can calculate RMSE here\n",
    "# rmse_profile = np.sqrt(mean_squared_error(actual_sps_profile, predicted_sps_profile))\n",
    "# print(f\"RMSE for profile data: {rmse_profile}\")\n",
    "\n",
    "profile_data['Pred_SP'] = predicted_sps_profile\n",
    "\n",
    "df_new['Predicted_SP'] = predicted_sps_profile\n",
    "df_new.to_csv(os.path.join(folder_path_out, Station + '_modelled.csv'), index=False)\n",
    "\n",
    "profile_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c3b58",
   "metadata": {},
   "source": [
    "## plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsw\n",
    "df_plot = df[['p','tnc90C','SP']] # Salinity_PSU #Conductivity_mScm\n",
    "#df_plot['Sal'] = gsw.SP_from_C(df_plot['Conductivity_mScm'], df_plot['Temp'], df_plot['Pressure_dbar'])\n",
    "pmax = df_plot['p'].max() +200\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "    #axs[0].plot(df_new['SP'],  df_new['depth'])\n",
    "axs[0].plot(profile_data['Pred_SP'], profile_data['pressure'])\n",
    "axs[0].plot(df_plot['SP'], df_plot['p'], color = 'red')\n",
    "axs[0].set_ylim(1000, pmax)\n",
    "axs[0].invert_yaxis()\n",
    "axs[0].set_ylabel(\"Pressure [dbar]\")\n",
    "axs[0].set_xlabel('salinity (PSU)')\n",
    "axs[0].set_xlabel('SP')\n",
    "axs[0].grid(True)\n",
    "\n",
    "\n",
    "axs[1].plot(profile_data['temp'], profile_data['pressure'])\n",
    "axs[1].set_ylim(1000, pmax)\n",
    "axs[1].invert_yaxis()\n",
    "axs[1].set_ylabel(\"Pressure [dbar]\")\n",
    "axs[1].set_xlabel('Temperature (°C)')\n",
    "axs[1].grid(True)\n",
    "\n",
    "    #figure_file = '/Users/jesskolbusz/Desktop/Full_data/figs/'\n",
    "\n",
    "fig.align_labels()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(os.path.join(figure_file, Station + '_modelled_' + cov_type + '.png'), dpi=300)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8c2d7",
   "metadata": {},
   "source": [
    "## evaluation details out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeba472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store actual and predicted salinity values\n",
    "actual_sps_eval = []\n",
    "predicted_sps_eval = []\n",
    "\n",
    "# Iterate over the evaluation data to predict salinity\n",
    "for index, row in eval_data.iterrows():\n",
    "    t_real = row['temp']\n",
    "    p_real = row['pressure']\n",
    "    actual_sp = row['salinity']\n",
    "    \n",
    "    # Predict salinity using the predict_SP function\n",
    "    predicted_sp = predict_SP(t_real, p_real, gmm, train_data)\n",
    "    \n",
    "    # Append actual and predicted salinity values to lists\n",
    "    actual_sps_eval.append(actual_sp)\n",
    "    predicted_sps_eval.append(predicted_sp)\n",
    "\n",
    "# Calculate RMSE for the evaluation set\n",
    "rmse_eval = np.sqrt(mean_squared_error(actual_sps_eval, predicted_sps_eval))\n",
    "print(f\"RMSE for evaluation data: {rmse_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GMM information and store in variables\n",
    "means = gmm.means_\n",
    "covariances = gmm.covariances_\n",
    "weights = gmm.weights_\n",
    "\n",
    "# Prepare the GMM info as strings to store in the DataFrame\n",
    "means_str = json.dumps(means.tolist())\n",
    "covariances_str = json.dumps(covariances.tolist())\n",
    "weights_str = json.dumps(weights.tolist())\n",
    "\n",
    "# Append the new results to the results DataFrame\n",
    "new_row = pd.DataFrame([{\n",
    "    'Station': Station,\n",
    "    'Type of Covariance': cov_type,\n",
    "    'Number of Clusters': n_components,\n",
    "    'RMSE': rmse_eval,\n",
    "    'Means': means_str,\n",
    "    'Covariances': covariances_str,\n",
    "    'Weights': weights_str\n",
    "}])\n",
    "\n",
    "###### Append the new results to the results DataFramed\n",
    "#### DOESNT CONCAT PROPERLY???? ###\n",
    "results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "results_df.to_csv(os.path.join(folder_path_out, 'Results_means.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbeca3",
   "metadata": {},
   "source": [
    "#   Plot all out modelled profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import windows, convolve\n",
    "# ---- helper: NaN-safe convolution along 1D ----\n",
    "def smooth_nan_safe(y, win_len=11):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    w = windows.hann(win_len)\n",
    "    w = w / w.sum()\n",
    "    valid = np.isfinite(y).astype(float)\n",
    "    y_filled = np.where(np.isfinite(y), y, 0.0)\n",
    "    num = convolve(y_filled, w, mode='same')\n",
    "    den = convolve(valid,   w, mode='same')\n",
    "    out = num / np.where(den > 1e-12, den, np.nan)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918535c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_profiles = []\n",
    "path_start = '/Users/00096315/Library/CloudStorage/OneDrive-UWA/01 Work/03. Python/Projects/NOV/ML_profiles/gmm_out'\n",
    "win_len = 11  # ~20 dbar at 2 dbar spacing\n",
    "\n",
    "for files in os.listdir(path_start):\n",
    "    if files.endswith('_modelled.csv'):\n",
    "        fpath = os.path.join(path_start, files)\n",
    "        try:\n",
    "            df = pd.read_csv(fpath)\n",
    "\n",
    "            Station = fpath.split('/')[-1].replace('_modelled.csv', '')  # Add station name\n",
    "            df['Station']  = Station\n",
    "            filtered_dep_log = dep_log[dep_log['Station'] == Station]\n",
    "\n",
    "            target_lat = filtered_dep_log['Lat'].iloc[0]\n",
    "            target_lon = filtered_dep_log['Lon'].iloc[0]\n",
    "\n",
    "            win = windows.hann(11)\n",
    "            df['t_smooth']  = smooth_nan_safe(df['temp'].values, win_len)\n",
    "\n",
    "            df['SA'] = gsw.SA_from_SP(df['Predicted_SP'].values, df['pressure'].values, target_lon, target_lat)  # Replace lon=0, lat=0 with your actual coordinates\n",
    "            df['CT'] = gsw.CT_from_t(df['SA'].values, df['t_smooth'].values, df['pressure'].values)\n",
    "\n",
    "            all_profiles.append(df[['SA', 'CT', 'pressure', 'Station']])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fpath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "plot_df = pd.concat(all_profiles, ignore_index=True)\n",
    "\n",
    "# (Optional) drop rows with missing SA/CT\n",
    "plot_df = plot_df.dropna(subset=['SA', 'CT'])\n",
    "\n",
    "# Plot with Plotly (use scattergl if very large)\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x='SA',\n",
    "    y='CT',\n",
    "    color='Station',\n",
    "    hover_data={'Station': True, 'SA': ':.3f', 'CT': ':.3f'},\n",
    "    title='CTD Profiles —  Nova Canton Leg 2',\n",
    "    render_mode='webgl'  # faster for many points\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Absolute Salinity (g/kg)'), #, range=[33.5, 35.0]),\n",
    "    yaxis=dict(title='Conservative Temperature (°C)'), #, range=[-1.5, 2.5]),\n",
    "    legend_title='Station'\n",
    ")\n",
    "\n",
    "pio.renderers.default = 'browser'\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
